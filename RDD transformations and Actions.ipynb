{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Think of it for a moment – 1 Qunitillion = 1 Million Billion! Can you imagine how many drives / CDs / Blue-ray DVDs would be required to store them? It is difficult to imagine this scale of data generation even as a data science professional. While this pace of data generation is very exciting,  it has created entirely new set of challenges and has forced us to find new ways to handle Big Huge data effectively.',\n",
       " '',\n",
       " 'Big Data is not a new phenomena. It has been around for a while now. However, it has become really important with this pace of data generation. In past, several systems were developed for processing big data. Most of them were based on MapReduce framework. These frameworks typically rely on use of hard disk for saving and retrieving the results. However, this turns out to be very costly in terms of time and speed.',\n",
       " '',\n",
       " 'On the other hand, Organizations have never been more hungrier to add a competitive differentiation through understanding this data and offering its customer a much better experience. Imagine how valuable would be Facebook, if it did not understand your interests well? The traditional hard disk based MapReduce kind of frameworks do not help much to address this challenge.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile('blogtexts')\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation: map and flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['think',\n",
       "  'of',\n",
       "  'it',\n",
       "  'for',\n",
       "  'a',\n",
       "  'moment',\n",
       "  '–',\n",
       "  '1',\n",
       "  'qunitillion',\n",
       "  '=',\n",
       "  '1',\n",
       "  'million',\n",
       "  'billion!',\n",
       "  'can',\n",
       "  'you',\n",
       "  'imagine',\n",
       "  'how',\n",
       "  'many',\n",
       "  'drives',\n",
       "  '/',\n",
       "  'cds',\n",
       "  '/',\n",
       "  'blue-ray',\n",
       "  'dvds',\n",
       "  'would',\n",
       "  'be',\n",
       "  'required',\n",
       "  'to',\n",
       "  'store',\n",
       "  'them?',\n",
       "  'it',\n",
       "  'is',\n",
       "  'difficult',\n",
       "  'to',\n",
       "  'imagine',\n",
       "  'this',\n",
       "  'scale',\n",
       "  'of',\n",
       "  'data',\n",
       "  'generation',\n",
       "  'even',\n",
       "  'as',\n",
       "  'a',\n",
       "  'data',\n",
       "  'science',\n",
       "  'professional.',\n",
       "  'while',\n",
       "  'this',\n",
       "  'pace',\n",
       "  'of',\n",
       "  'data',\n",
       "  'generation',\n",
       "  'is',\n",
       "  'very',\n",
       "  'exciting,',\n",
       "  'it',\n",
       "  'has',\n",
       "  'created',\n",
       "  'entirely',\n",
       "  'new',\n",
       "  'set',\n",
       "  'of',\n",
       "  'challenges',\n",
       "  'and',\n",
       "  'has',\n",
       "  'forced',\n",
       "  'us',\n",
       "  'to',\n",
       "  'find',\n",
       "  'new',\n",
       "  'ways',\n",
       "  'to',\n",
       "  'handle',\n",
       "  'big',\n",
       "  'huge',\n",
       "  'data',\n",
       "  'effectively.'],\n",
       " [],\n",
       " ['big',\n",
       "  'data',\n",
       "  'is',\n",
       "  'not',\n",
       "  'a',\n",
       "  'new',\n",
       "  'phenomena.',\n",
       "  'it',\n",
       "  'has',\n",
       "  'been',\n",
       "  'around',\n",
       "  'for',\n",
       "  'a',\n",
       "  'while',\n",
       "  'now.',\n",
       "  'however,',\n",
       "  'it',\n",
       "  'has',\n",
       "  'become',\n",
       "  'really',\n",
       "  'important',\n",
       "  'with',\n",
       "  'this',\n",
       "  'pace',\n",
       "  'of',\n",
       "  'data',\n",
       "  'generation.',\n",
       "  'in',\n",
       "  'past,',\n",
       "  'several',\n",
       "  'systems',\n",
       "  'were',\n",
       "  'developed',\n",
       "  'for',\n",
       "  'processing',\n",
       "  'big',\n",
       "  'data.',\n",
       "  'most',\n",
       "  'of',\n",
       "  'them',\n",
       "  'were',\n",
       "  'based',\n",
       "  'on',\n",
       "  'mapreduce',\n",
       "  'framework.',\n",
       "  'these',\n",
       "  'frameworks',\n",
       "  'typically',\n",
       "  'rely',\n",
       "  'on',\n",
       "  'use',\n",
       "  'of',\n",
       "  'hard',\n",
       "  'disk',\n",
       "  'for',\n",
       "  'saving',\n",
       "  'and',\n",
       "  'retrieving',\n",
       "  'the',\n",
       "  'results.',\n",
       "  'however,',\n",
       "  'this',\n",
       "  'turns',\n",
       "  'out',\n",
       "  'to',\n",
       "  'be',\n",
       "  'very',\n",
       "  'costly',\n",
       "  'in',\n",
       "  'terms',\n",
       "  'of',\n",
       "  'time',\n",
       "  'and',\n",
       "  'speed.'],\n",
       " [],\n",
       " ['on',\n",
       "  'the',\n",
       "  'other',\n",
       "  'hand,',\n",
       "  'organizations',\n",
       "  'have',\n",
       "  'never',\n",
       "  'been',\n",
       "  'more',\n",
       "  'hungrier',\n",
       "  'to',\n",
       "  'add',\n",
       "  'a',\n",
       "  'competitive',\n",
       "  'differentiation',\n",
       "  'through',\n",
       "  'understanding',\n",
       "  'this',\n",
       "  'data',\n",
       "  'and',\n",
       "  'offering',\n",
       "  'its',\n",
       "  'customer',\n",
       "  'a',\n",
       "  'much',\n",
       "  'better',\n",
       "  'experience.',\n",
       "  'imagine',\n",
       "  'how',\n",
       "  'valuable',\n",
       "  'would',\n",
       "  'be',\n",
       "  'facebook,',\n",
       "  'if',\n",
       "  'it',\n",
       "  'did',\n",
       "  'not',\n",
       "  'understand',\n",
       "  'your',\n",
       "  'interests',\n",
       "  'well?',\n",
       "  'the',\n",
       "  'traditional',\n",
       "  'hard',\n",
       "  'disk',\n",
       "  'based',\n",
       "  'mapreduce',\n",
       "  'kind',\n",
       "  'of',\n",
       "  'frameworks',\n",
       "  'do',\n",
       "  'not',\n",
       "  'help',\n",
       "  'much',\n",
       "  'to',\n",
       "  'address',\n",
       "  'this',\n",
       "  'challenge.']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Func(lines):\n",
    "    lines = lines.lower()\n",
    "    lines = lines.split()\n",
    "    return lines\n",
    "rdd1 = rdd.map(Func)\n",
    "rdd1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['think', 'of', 'it', 'for', 'a']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = rdd.flatMap(Func)\n",
    "rdd2.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation: filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['think', 'of', 'it', 'moment', '–', '1', 'qunitillion', '=', '1', 'million']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = ['is','am','are','the','for','a']\n",
    "rdd3 = rdd2.filter(lambda x: x not in stopwords)\n",
    "rdd3.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation: groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('thi', ['think', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'think', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'things', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this.', 'this', 'this', 'things', 'this', 'this', 'this'])]\n"
     ]
    }
   ],
   "source": [
    "rdd4 = rdd3.groupBy(lambda w:w[0:3])\n",
    "print([(k,list(v)) for (k,v) in rdd4.take(1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation: groupByKey/reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('think', [1, 1]), ('of', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), ('1', [1, 1]), ('qunitillion', [1]), ('=', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])]\n"
     ]
    }
   ],
   "source": [
    "rdd3_mapped = rdd3.map(lambda x:(x,1))\n",
    "rdd3_grouped = rdd3_mapped.groupByKey()\n",
    "print(list((j[0],list(j[1]))  for j in rdd3_grouped.take(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3_freq_of_words = rdd3_grouped.mapValues(sum) \\\n",
    "                .map(lambda x:(x[1],x[0])) \\\n",
    "                .sortByKey(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, I first applied “**mapValues**” transformation on “rdd3_grouped”. The “mapValues” (only applicable on pair RDD) transformation is like a map (can be applied on any RDD) transform but it has one difference that when we apply map transform on pair RDD we can access the key and value both of this RDD but in case of “mapValues” transformation, it will transform the values by applying some function and key will not be affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(164, 'to'),\n",
       " (143, 'in'),\n",
       " (122, 'of'),\n",
       " (106, 'and'),\n",
       " (103, 'we'),\n",
       " (69, 'spark'),\n",
       " (64, 'this'),\n",
       " (63, 'data'),\n",
       " (55, 'can'),\n",
       " (52, 'apache')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3_freq_of_words.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(164, 'to'),\n",
       " (143, 'in'),\n",
       " (122, 'of'),\n",
       " (106, 'and'),\n",
       " (103, 'we'),\n",
       " (69, 'spark'),\n",
       " (64, 'this'),\n",
       " (63, 'data'),\n",
       " (55, 'can'),\n",
       " (52, 'apache')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3_mapped.reduceByKey(lambda x,y:x+y) \\\n",
    "        .map(lambda x:(x[1],x[0])) \\\n",
    "        .sortByKey(False).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![reducebykey](https://www.analyticsvidhya.com/wp-content/uploads/2016/10/reduceByKey-3.png)\n",
    "![groupbykey](https://www.analyticsvidhya.com/wp-content/uploads/2016/10/groupbykey.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation: mapPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[49, 39], [20, 13]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def func(iterator):\n",
    "    count_spark = 0\n",
    "    count_apache = 0\n",
    "    for i in iterator:\n",
    "        if i=='spark':\n",
    "            count_spark += 1\n",
    "        if i=='apache':\n",
    "            count_apache += 1\n",
    "            \n",
    "    return(count_spark,count_apache)\n",
    "\n",
    "rdd3.mapPartitions(func).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used the “**glom**” function which is very useful when we want to see the data insights for each partition of a RDD. So above result shows that 49,39 are the counts of ‘spark’, ‘apache’ in partition1 and 20,13 are the counts of ‘spark’, ‘apache’ in partition2. If we won’t use the “glom” function we won’t we able to see the results of each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[49, 39, 20, 13]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.mapPartitions(func).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math/Statistical Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation: sample\n",
    "We can pass the arguments insights as the sample operation:\n",
    "\n",
    "“**withReplacement** = True” or False (to choose the sample with or without replacement)\n",
    "“**fraction** = x” ( x= .4 means we want to choose 40% of data in “rdd” ) and “seed” for reproduce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4768, 1872)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3_sampled = rdd3.sample(False,0.4,42)\n",
    "len(rdd3.collect()),len(rdd3_sampled.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Theory/Relational Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation: union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(931, 931, 1862)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1 = rdd3.sample(False,0.2,42)\n",
    "sample2 = rdd3.sample(False,0.2,42)\n",
    "union_of_sample1_sample2 = sample1.union(sample2)\n",
    "len(sample1.collect()), len(sample2.collect()),len(union_of_sample1_sample2.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "union operation didn’t remove the duplicate elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation: join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('think', (1, 1)), ('even', (1, 1))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1 = rdd3_mapped.sample(False,.2,42)\n",
    "sample2 = rdd3_mapped.sample(False,.2,42)\n",
    "join_on_sample1_sample2 = sample1.join(sample2)\n",
    "join_on_sample1_sample2.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation: distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1485"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3_distinct = rdd3.distinct()\n",
    "len(rdd3_distinct.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure/I/O Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation: coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3_coalesce = rdd3.coalesce(1)\n",
    "rdd3_coalesce.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action: getNumPartitions\n",
    "With “getNumPartitions”, we can find out that how many partitions exist in our RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action: Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499500"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd = sc.parallelize(range(1,1000))\n",
    "num_rdd.reduce(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical/Statistical Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action: count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4768"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action: max,min,sum,variance and stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 1, 499500, 83166.66666666667, 288.38631497813253)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd.max(),num_rdd.min(),num_rdd.sum(),num_rdd.variance(),num_rdd.stdev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
